{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "cse415A6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6QA1D6eIszX_"
      },
      "source": [
        "# CSE 415 Deep Learning Assignment (A6)\n",
        "Please go through the entire assignment, and read all lines of code, including the comments. However, only changes to areas of the code marked \"**TO DO**\" will be graded.\n",
        "\n",
        "# Part 0: Pytorch Setup Code\n",
        "Write your name where designated. Format: First name followed by last name with a space in between, and initials capitalized. For example:\n",
        "\n",
        "name = 'Bindita Chaudhuri'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DHxEgxT6YwR8",
        "colab": {}
      },
      "source": [
        "# This shows how to connect your google drive account with a colab instance.\n",
        "\n",
        "# Load the Drive helper and mount; this will prompt for authorization \n",
        "# (Login to google account; allow access, copy the code and paste it below and then press enter)\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "# create a folder named CSE415 in your Drive\n",
        "import os \n",
        "BASE_PATH = '/gdrive/My Drive/CSE415/'\n",
        "if not os.path.exists(BASE_PATH):\n",
        "    os.makedirs(BASE_PATH)\n",
        "    \n",
        "# now let's test that Google Drive is up and running. \n",
        "!ls \"/gdrive/My Drive/CSE415\"\n",
        "\n",
        "# The following line will create a text file \"foo.txt\" in the created folder and then remove it\n",
        "!echo \"Hello Google Drive\" > \"/gdrive/My Drive/CSE415/foo.txt\"\n",
        "!cat \"/gdrive/My Drive/CSE415/foo.txt\"\n",
        "!rm \"/gdrive/My Drive/CSE415/foo.txt\"\n",
        "\n",
        "import torch\n",
        "print('Version', torch.__version__)\n",
        "print('CUDA enabled:', torch.cuda.is_available())\n",
        "  \n",
        "# Running this should then print out:\n",
        "# Version 1.3.1\n",
        "# CUDA enabled: True\n",
        "\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import glob, re, pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import h5py, tqdm\n",
        "import sys\n",
        "sys.path.append(BASE_PATH)\n",
        "\n",
        "# TO DO\n",
        "# write your name here in the format mentioned above\n",
        "name = 'Bindita Chaudhuri'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lrYXOGpsM6TV",
        "colab": {}
      },
      "source": [
        "# Some useful save, restoring and conversion functions are provided below. No need to change them.\n",
        "\n",
        "!pip3 install matplotlib-label-lines\n",
        "from labellines import labelLines\n",
        "\n",
        "class pt_util(object):\n",
        "    # This does more than the simple Pytorch restore. It checks that the names \n",
        "    # of variables match, and if they don't doesn't throw a fit. It is similar \n",
        "    # to how Caffe acts. This is especially useful if you decide to change your\n",
        "    # network architecture but don't want to retrain from scratch.\n",
        "    @staticmethod\n",
        "    def restore(net, save_file):\n",
        "        \"\"\" Args:\n",
        "        net(torch.nn.Module): The net to restore\n",
        "        save_file(str): The file path\n",
        "        \"\"\"\n",
        "        net_state_dict = net.state_dict()\n",
        "        restore_state_dict = torch.load(save_file)\n",
        "\n",
        "        restored_var_names = set()\n",
        "\n",
        "        print('Restoring:')\n",
        "        for var_name in restore_state_dict.keys():\n",
        "            if var_name in net_state_dict:\n",
        "                var_size = net_state_dict[var_name].size()\n",
        "                restore_size = restore_state_dict[var_name].size()\n",
        "                if var_size != restore_size:\n",
        "                    print('Shape mismatch for var', var_name, 'expected', var_size, 'got', restore_size)\n",
        "                else:\n",
        "                    if isinstance(net_state_dict[var_name], torch.nn.Parameter):\n",
        "                        # backwards compatibility for serialized parameters\n",
        "                        net_state_dict[var_name] = restore_state_dict[var_name].data\n",
        "                    try:\n",
        "                        net_state_dict[var_name].copy_(restore_state_dict[var_name])\n",
        "                        print(str(var_name)+' -> \\t'+str(var_size)+' = '+str(int(np.prod(var_size)*4 / 10**6)) + 'MB')\n",
        "                        restored_var_names.add(var_name)\n",
        "                    except:\n",
        "                        print('While copying the parameter named {}, whose dimensions in the model are'\n",
        "                              ' {} and whose dimensions in the checkpoint are {}, ...'.format(\n",
        "                                  var_name, var_size, restore_size))\n",
        "                        raise\n",
        "\n",
        "        ignored_var_names = sorted(list(set(restore_state_dict.keys()) - restored_var_names))\n",
        "        unset_var_names = sorted(list(set(net_state_dict.keys()) - restored_var_names))\n",
        "        print('')\n",
        "        if len(ignored_var_names) == 0:\n",
        "            print('Restored all variables')\n",
        "        else:\n",
        "            print('Did not restore:\\n\\t' + '\\n\\t'.join(ignored_var_names))\n",
        "        if len(unset_var_names) == 0:\n",
        "            print('No new variables')\n",
        "        else:\n",
        "            print('Initialized but did not modify:\\n\\t' + '\\n\\t'.join(unset_var_names))\n",
        "\n",
        "        print('Restored %s' % save_file)\n",
        "        \n",
        "    # Restores the last saved network in a folder using file write time.\n",
        "    @staticmethod\n",
        "    def restore_latest(net, folder):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          net(torch.nn.module): The net to restore\n",
        "          folder(str): The folder path\n",
        "        Returns:\n",
        "          int: Attempts to parse the epoch from the state and returns it if possible. Otherwise returns 0.\n",
        "        \"\"\"\n",
        "        checkpoints = sorted(glob.glob(folder + '/*.pt'), key=os.path.getmtime)\n",
        "        start_it = 0\n",
        "        if len(checkpoints) > 0:\n",
        "            pt_util.restore(net, checkpoints[-1])\n",
        "            start_it = int(re.findall(r'\\d+', checkpoints[-1])[-1])\n",
        "        return start_it\n",
        "\n",
        "    # Saves the network and optionally deletes old save files. \n",
        "    # If num_to_keep is 0, it won't remove any.\n",
        "    @staticmethod\n",
        "    def save(net, file_name, num_to_keep=1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "        net(torch.nn.module): The network to save\n",
        "        file_name(str): the path to save the file.\n",
        "        num_to_keep(int): Specifies how many previous saved states to keep once this one has been saved.\n",
        "            Defaults to 1. Specifying < 0 will not remove any previous saves.\n",
        "        \"\"\"\n",
        "        folder = os.path.dirname(file_name)\n",
        "        if not os.path.exists(folder):\n",
        "            os.makedirs(folder)\n",
        "        torch.save(net.state_dict(), file_name)\n",
        "        extension = os.path.splitext(file_name)[1]\n",
        "        checkpoints = sorted(glob.glob(folder + '/*' + extension), key=os.path.getmtime)\n",
        "        print('Saved %s\\n' % file_name)\n",
        "        if num_to_keep > 0:\n",
        "            for ff in checkpoints[:-num_to_keep]:\n",
        "                os.remove(ff)\n",
        "                \n",
        "\n",
        "    @staticmethod\n",
        "    def to_numpy(array):\n",
        "        if isinstance(array, torch.Tensor):\n",
        "            return array.detach().cpu().numpy()\n",
        "        elif isinstance(array, dict):\n",
        "            return {key: pt_util.to_numpy(val) for key, val in array.items()}\n",
        "        else:\n",
        "            return np.asarray(array)\n",
        "\n",
        "    @staticmethod\n",
        "    def from_numpy(np_array):\n",
        "        if isinstance(np_array, list):\n",
        "            try:\n",
        "                np_array = np.stack(np_array, 0)\n",
        "            except ValueError:\n",
        "                np_array = np.stack([from_numpy(val) for val in np_array], 0)\n",
        "        elif isinstance(np_array, dict):\n",
        "            return {key: from_numpy(val) for key, val in np_array.items()}\n",
        "        np_array = np.asarray(np_array)\n",
        "        if np_array.dtype == np.uint32:\n",
        "            print(\"numpy -> torch dtype uint32 not supported, using int32\")\n",
        "            np_array = np_array.astype(np.int32)\n",
        "        elif np_array.dtype == np.dtype(\"O\"):\n",
        "            print(\"numpy -> torch dtype Object not supported, returning numpy array\")\n",
        "            return np_array\n",
        "        elif np_array.dtype.type == np.str_:\n",
        "            print(\"numpy -> torch dtype numpy.str_ not supported, returning numpy array\")\n",
        "            return np_array\n",
        "        return torch.from_numpy(np_array)\n",
        "\n",
        "    @staticmethod\n",
        "    def write_log(filename, data):\n",
        "        \"\"\"Pickles and writes data to a file\n",
        "        Args:\n",
        "            filename(str): File name\n",
        "            data(pickleable object): Data to save\n",
        "        \"\"\"\n",
        "        if not os.path.exists(os.path.dirname(filename)):\n",
        "          os.makedirs(os.path.dirname(filename))\n",
        "        pickle.dump(data, open(filename, 'wb')) \n",
        "\n",
        "    def read_log(filename, default_value=None):\n",
        "        \"\"\"Reads pickled data or returns the default value if none found\n",
        "        Args:\n",
        "            filename(str): File name\n",
        "            default_value(anything): Value to return if no file is found\n",
        "        Returns:\n",
        "            unpickled file\n",
        "        \"\"\"\n",
        "        if os.path.exists(filename):\n",
        "            return pickle.load(open(filename, 'rb'))\n",
        "        return default_value\n",
        "    \n",
        "    # Create plots\n",
        "    @staticmethod\n",
        "    def plot(x_values, y_values, title, xlabel, ylabel, plotlabel):\n",
        "        \"\"\"Plots a line graph\n",
        "        Args:\n",
        "            x_values(list or np.array): x values for the line\n",
        "            y_values(list or np.array): y values for the line\n",
        "            title(str): Title for the plot\n",
        "            xlabel(str): Label for the x axis\n",
        "            ylabel(str): label for the y axis\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(20, 10))\n",
        "        plt.plot(x_values, y_values, label=plotlabel, color='b')\n",
        "        labelLines(plt.gca().get_lines(), color='b',fontsize=13)\n",
        "        plt.title(title)\n",
        "        plt.xlabel(xlabel)\n",
        "        plt.ylabel(ylabel)\n",
        "        plt.show()\n",
        "\n",
        "    def to_scaled_uint8(array):\n",
        "        \"\"\"Returns a normalized uint8 scaled to 0-255. This is useful for showing images especially of floats.\n",
        "        Args:\n",
        "            array(np.array): The array to normalize\n",
        "        Returns:\n",
        "            np.array normalized and of type uint8\n",
        "        \"\"\"\n",
        "        array = np.array(array, dtype=np.float32)\n",
        "        array -= np.min(array)\n",
        "        array *= (255. / np.max(array))\n",
        "        array = array.astype(np.uint8)\n",
        "        return array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u486U-pUJnDY"
      },
      "source": [
        "# Part 1: Classification Network for CIFAR-10\n",
        "\n",
        "# 1.1 Loading and transforming data\n",
        "\n",
        "CIFAR-10 data consists of 60000 32x32 images, belonging to 10 classes. More details can be found here:\n",
        "https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "\n",
        "Note: Each image is a matrix of shape width x height (W x H) with 3 channels (C).\n",
        "\n",
        "The code for loading the dataset and transforming the data accordingly is provided. \n",
        "The transform *ToTensor* converts the output from the dataset to be a tensor in CxHxW format.\n",
        "One type of data augmentation - data normalization, is shown in the transforms. You have to add at least 2 of the following data augmentations in 'transform_train' (try to understand which ones you need, and what parameters you should choose for them):\n",
        "\n",
        "- RandomHorizontalFlip\n",
        "- RandomCrop\n",
        "- ColorJitter\n",
        "- RandomRotation\n",
        "\n",
        "\n",
        "# 1.2 Defining network\n",
        "\n",
        "You need to define a convolutional neural network (CNN) to classify the data into its classes. Sample code for classification of MNIST (handwritten digits dataset) is given here: https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
        "\n",
        "You will define the network layers in the `__init__` function, and the forward call will pass the data through those layers. The network should be as follows:\n",
        "\n",
        "- One 3x3 convolution layer with 32 filters.\n",
        "- One 2x2 maxpooling layer\n",
        "- One 3x3 convolution layer with 64 filters. \n",
        "- One 3x3 convolution layer with 64 filters. \n",
        "- One 2x2 maxpooling layer\n",
        "- One 3x3 convolution layer with 128 filters. \n",
        "- One 3x3 convolution layer with 128 filters.\n",
        "- One 2x2 maxpooling layer\n",
        "- One fully connected layer with 512 outputs.\n",
        "- Then the final classification layer with 10 outputs.\n",
        "\n",
        "- Every convolution layer should be followed by ReLU nonlinearity followed by batch normalization. Use padding in every convolution layer to retain input image size.\n",
        "        \n",
        "\n",
        "\n",
        "# Helpful functions\n",
        "- https://pytorch.org/docs/stable/tensors.html\n",
        "- https://pytorch.org/docs/stable/nn.html\n",
        "- You can call `pt_util.to_numpy(x)` to get a numpy array from a torch tensor x.\n",
        "- `pt_util.from_numpy(x)` makes a torch Tensor from the numpy array x.\n",
        "\n",
        "# Common Oopsies\n",
        "- __Q__ It only runs for one iteration and says it's done: __A__ We provided code that automatically loads the most recent file. If you don't want to start from that checkpoint, simply find it in your google drive and delete it.\n",
        "- __Q__ I want to save more than just the last checkpoint: __A__ You can change the save function to save any number of previous checkpoints. You can also tell it to save all of them (not delete anything), by passing in 0.\n",
        "- __Q__ Pytorch is saying things are the wrong shape: __A__ You can easily reshape things using the `view` function (https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view). It is like the Numpy `reshape` function.\n",
        "- __Q__ Pytorch is saying things are on the wrong device: __A__ You can move data between devices with the `.to(device)` call. Generally, all arguments to a function will need to be on the same device."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0oKKR9EsoK9G",
        "colab": {}
      },
      "source": [
        "# Data augmentation\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    # TO DO\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# Load the dataset\n",
        "\n",
        "DATA_PATH = BASE_PATH + 'cifar10/'\n",
        "data_train = datasets.CIFAR10(root=DATA_PATH, train=True, download=True, transform=transform_train)\n",
        "data_test = datasets.CIFAR10(root=DATA_PATH, train=False, download=True, transform=transform_test)\n",
        "\n",
        "\n",
        "# This is where you define your network architecture.\n",
        "# You can use that as a guide, but make sure you understand what it all does.\n",
        "\n",
        "class CifarNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CifarNet, self).__init__()\n",
        "        self.best_accuracy = -1\n",
        "        # TO DO\n",
        "        raise NotImplementedError('Define the layers here')\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        # TO DO\n",
        "        raise NotImplementedError('Define the forward pass')\n",
        "        \n",
        "      \n",
        "    def loss(self, prediction, label, reduction='elementwise_mean'):\n",
        "        loss_val = F.cross_entropy(prediction, label.squeeze(), reduction=reduction)\n",
        "        return loss_val\n",
        "\n",
        "    def save_model(self, file_path, num_to_keep=1):\n",
        "        pt_util.save(self, file_path, num_to_keep)\n",
        "        \n",
        "    def save_best_model(self, accuracy, file_path, num_to_keep=1):\n",
        "        if accuracy > self.best_accuracy:\n",
        "            self.best_accuracy = accuracy\n",
        "            self.save_model(file_path, num_to_keep)\n",
        "\n",
        "    def load_model(self, file_path):\n",
        "        pt_util.restore(self, file_path)\n",
        "\n",
        "    def load_last_model(self, dir_path):\n",
        "        return pt_util.restore_latest(self, dir_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIYXQW8fX8gb",
        "colab_type": "text"
      },
      "source": [
        "# 1.3 Training and testing the network\n",
        "\n",
        "The train and test functions are given below. Fill in the TO DO with the given instructions. At the end, training and testing loss and accuracy plots will be displayed. Right click on the plots and save them as png files. Put these images on a single page (titled **CIFAR plots**) in your report, which you can create as a doc file but you will need to convert it into a pdf for submission. You also need to submit the 'cifar_050.pt' file from the 'checkpoints' folder inside your 'cifar10' folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pNf3AoHVvKXI",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch, log_interval):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for batch_idx, (data, label) in enumerate(train_loader):\n",
        "        # send the data and labels to GPU\n",
        "        data, label = data.to(device), label.to(device)\n",
        "        # initialize the optimizer\n",
        "        optimizer.zero_grad()\n",
        "        # TO DO (call the 'model' with 'data' as input)\n",
        "        output = None\n",
        "        # TO DO (call the model.loss function with 'output' and 'label' as inputs) \n",
        "        loss = None\n",
        "        \n",
        "        # backpropagate the loss\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Log the training progress\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('{} Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                time.ctime(time.time()),\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "    return np.mean(losses)\n",
        "\n",
        "def test(model, device, test_loader, log_interval=None):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, label) in enumerate(test_loader):\n",
        "            # send data and label to GPU\n",
        "            data, label = data.to(device), label.to(device)\n",
        "            # TO DO (call the model with 'data' as input)\n",
        "            output = None\n",
        "            # TO DO (call model.loss function with 'output and 'label' as inputs and reduction='sum')\n",
        "            test_loss_on = None\n",
        "            \n",
        "            test_loss += test_loss_on.item()\n",
        "            \n",
        "            # Take the class with maximum probability as the output class.\n",
        "            pred = output.max(1)[1]\n",
        "            correct_mask = pred.eq(label.view_as(pred))\n",
        "            num_correct = correct_mask.sum().item()\n",
        "            correct += num_correct\n",
        "            \n",
        "            # Log the test progress\n",
        "            if log_interval is not None and batch_idx % log_interval == 0:\n",
        "                print('{} Test: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    time.ctime(time.time()),\n",
        "                    batch_idx * len(data), len(test_loader.dataset),\n",
        "                    100. * batch_idx / len(test_loader), test_loss_on))\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset), test_accuracy))\n",
        "    return test_loss, test_accuracy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "50zDbqjXu_Qq",
        "colab": {}
      },
      "source": [
        "# Now the actual training and testing code\n",
        "\n",
        "import multiprocessing\n",
        "import traceback\n",
        "\n",
        "# Play around with these constants, you may find a better setting.\n",
        "BATCH_SIZE = 32\n",
        "TEST_BATCH_SIZE = 32\n",
        "EPOCHS = 50\n",
        "LEARNING_RATE = 0.001\n",
        "MOMENTUM = 0.9\n",
        "USE_CUDA = True\n",
        "SEED = 0\n",
        "PRINT_INTERVAL = 100\n",
        "WEIGHT_DECAY = 0.0005\n",
        "LOG_PATH = DATA_PATH + 'log.pkl'\n",
        "\n",
        "# handle GPU connection and multiprocessing\n",
        "use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print('Using device', device)\n",
        "import multiprocessing\n",
        "print('num cpus:', multiprocessing.cpu_count())\n",
        "kwargs = {'num_workers': 0, 'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "class_names = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# iterator over the data\n",
        "train_loader = torch.utils.data.DataLoader(data_train, batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(data_test, batch_size=TEST_BATCH_SIZE, shuffle=False, **kwargs)\n",
        "\n",
        "# Initialize network and send it to GPU\n",
        "model = CifarNet().to(device)\n",
        "# Define optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# This will train from scratch\n",
        "start_epoch = 0 \n",
        "train_losses, test_losses, test_accuracies = [], [], []\n",
        "\n",
        "# To resume training from last saved model, uncomment the following 2 lines\n",
        "# start_epoch = model.load_last_model(DATA_PATH + 'checkpoints')\n",
        "# train_losses, test_losses, test_accuracies = pt_util.read_log(LOG_PATH, ([], [], []))\n",
        "\n",
        "# Get the initial test losses and accuracies\n",
        "test_loss, test_accuracy = test(model, device, test_loader)\n",
        "test_losses.append((start_epoch, test_loss))\n",
        "test_accuracies.append((start_epoch, test_accuracy))\n",
        "\n",
        "try:\n",
        "    for epoch in range(start_epoch, EPOCHS + 1):\n",
        "        # train the model for 1 epoch\n",
        "        train_loss = train(model, device, train_loader, optimizer, epoch, PRINT_INTERVAL)\n",
        "        train_losses.append((epoch, train_loss))\n",
        "        # test the model after 1 epoch\n",
        "        test_loss, test_accuracy = test(model, device, test_loader)\n",
        "        test_losses.append((epoch, test_loss))\n",
        "        test_accuracies.append((epoch, test_accuracy))\n",
        "        # Log the losses and accuracy\n",
        "        pt_util.write_log(LOG_PATH, (train_losses, test_losses, test_accuracies))\n",
        "        # save the current model in the checkpoints folder\n",
        "        model.save_best_model(test_accuracy, DATA_PATH + 'checkpoints/cifar_%03d.pt' % epoch)\n",
        "\n",
        "\n",
        "except KeyboardInterrupt as ke:\n",
        "    print('Interrupted')\n",
        "except:\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "finally:\n",
        "    model.save_model(DATA_PATH + 'checkpoints/cifar_%03d.pt' % epoch, 0)\n",
        "    # Plot the loss and accuracy values over epochs\n",
        "    ep, val = zip(*train_losses)\n",
        "    pt_util.plot(ep, val, 'Train loss', 'Epoch', 'Error', name)\n",
        "    ep, val = zip(*test_losses)\n",
        "    pt_util.plot(ep, val, 'Test loss', 'Epoch', 'Error', name)\n",
        "    ep, val = zip(*test_accuracies)\n",
        "    pt_util.plot(ep, val, 'Test accuracy', 'Epoch', 'Error', name)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDxG8IaHX8gm",
        "colab_type": "text"
      },
      "source": [
        "# 1.4 Attempt to use your own test data\n",
        "\n",
        "Download 5 images for each of the 4 classes given below. Choose the images randomly from Google, but make sure they look somewhat similar to the training data of CIFAR-10 in terms of content. For example, full body image of a dog with minimal background is recommended. Edit the text below to write down the links of the images (**NOTE**: The TAs will be evaluating YOUR network weights with YOUR chosen images, so do not collaborate with others regarding image choices.)\n",
        "\n",
        "Plane:\n",
        "\n",
        "- link 1\n",
        "- link 2\n",
        "- link 3\n",
        "- link 4\n",
        "- link 5\n",
        "\n",
        "Car:\n",
        "\n",
        "- link 1\n",
        "- link 2\n",
        "- link 3\n",
        "- link 4\n",
        "- link 5\n",
        "\n",
        "Dog:\n",
        "\n",
        "- link 1\n",
        "- link 2\n",
        "- link 3\n",
        "- link 4\n",
        "- link 5\n",
        "\n",
        "Horse:\n",
        "\n",
        "- link 1\n",
        "- link 2\n",
        "- link 3\n",
        "- link 4\n",
        "- link 5\n",
        "\n",
        "Now upload these images in the 'cifar10' folder in your drive. Then write a function show_images() to plot the images in a 4 x 5 grid using matplotlib. Functions to use among others:\n",
        "\n",
        "- plt.subplot() or fig.add_subplot()\n",
        "- plt.imshow() or ax.imshow()\n",
        "- plt.axis('off') or ax.set_axis_off()\n",
        "- plt.show()\n",
        "- Image.open() from PIL (convert to numpy array before use in imshow) \n",
        "\n",
        "Once done, right click on the grid and click 'Save Image As'. Put the image in your report on a page (titled **External Images**).\n",
        "\n",
        "# EXTRA CREDIT (20 points)\n",
        "Use these images as the test set and check the accuracy of your trained model on this custom test set. A useful article: https://towardsdatascience.com/building-efficient-custom-datasets-in-pytorch-2563b946fd9f\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Aud1m4OX8gn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TO DO : image visualization; you can modify anything here\n",
        "from PIL import Image\n",
        "def show_images():\n",
        "    raise NotImplementedError('Create the grid to visualize images here:')\n",
        "\n",
        "# EXTRA CREDIT CODE HERE (IF ANY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fj5IqkMljWwl"
      },
      "source": [
        "# Part 2: Language generation using character-level RNNs\n",
        "\n",
        "# 2.1 Processing the data\n",
        "\n",
        "We'll be using the complete text of Harry Potter as our corpus. Place the harry_potter.txt file provided in the 'language' folder created in your drive.\n",
        "\n",
        "1) Create a dictionary voc2ind to define the vocabulary. voc2ind contents will be like `{' ': 0, 'A': 1, 'B': 2, 'C': 3, ... '0': 35, '1': 36, ..., '$': 78, '#': 79, '(': 80, ...}`. The keys include all the 26 alphabets, both upper case and lower case separately, digits 0-9 and special characters that may appear in a general text. The values corresponding to the keys can be in any order. You may have to handle single and double quotes carefully in the keys since they are generally used as string delimiters. ind2voc is the inverse of voc2ind.\n",
        "\n",
        "2) Now using the vocabulary above, convert text into a list of tokens. For example, if the text is **`\"ABA CDBE\"`**, the token version will be a list with contents `[1, 2, 1, 0, 3, 4, 2, 5]`. Create this list where instructed below.\n",
        "\n",
        "3) train_text and test_text should contain the first 80% and the last 20% of the 'token' list. This should be simple to write.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD944kC5X8gr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_PATH = BASE_PATH + 'language/'\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    os.makedirs(DATA_PATH)\n",
        "\n",
        "def prepare_data(data_path):\n",
        "    with open(data_path) as f:\n",
        "        # This reads all the data from the file, but does not do any processing on it.\n",
        "        data = f.read()\n",
        "    \n",
        "    # TO DO (create the voc2ind dictionary)\n",
        "    voc2ind = {}\n",
        "    \n",
        "    # transform the data into an integer representation of the tokens.\n",
        "    token = []\n",
        "    for char in data:\n",
        "        # replaces all weird spacing like tab, next line etc. with space\n",
        "        if re.match('\\s+', char): char = ' '\n",
        "        # TO DO (create the list of tokens; basically replace None with the correct expression)\n",
        "        token.append(None)\n",
        "\n",
        "    ind2voc = {val: key for key, val in voc2ind.items()}\n",
        "\n",
        "    # TO DO \n",
        "    train_text = None\n",
        "    test_text = None\n",
        "\n",
        "    pickle.dump({'tokens': train_text, 'ind2voc': ind2voc, 'voc2ind':voc2ind}, open(DATA_PATH + 'harry_potter_chars_train.pkl', 'wb'))\n",
        "    pickle.dump({'tokens': test_text, 'ind2voc': ind2voc, 'voc2ind':voc2ind}, open(DATA_PATH + 'harry_potter_chars_test.pkl', 'wb'))\n",
        "    print('Data prepared!')\n",
        "    \n",
        "prepare_data(DATA_PATH + 'harry_potter.txt')\n",
        "\n",
        "class Vocabulary(object):\n",
        "    def __init__(self, data_file):\n",
        "        with open(data_file, 'rb') as data_file:\n",
        "            dataset = pickle.load(data_file)\n",
        "        self.ind2voc = dataset['ind2voc']\n",
        "        self.voc2ind = dataset['voc2ind']\n",
        "\n",
        "    # Returns a string representation of the tokens.\n",
        "    def array_to_words(self, arr):\n",
        "        return ''.join([self.ind2voc[int(ind)] for ind in arr])\n",
        "\n",
        "    # Returns a torch tensor representing each token in words.\n",
        "    def words_to_array(self, words):\n",
        "        return torch.LongTensor([self.voc2ind[word] for word in words])\n",
        "\n",
        "    # Returns the size of the vocabulary.\n",
        "    def __len__(self):\n",
        "        return len(self.voc2ind)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcjjN2MSgSm5",
        "colab_type": "text"
      },
      "source": [
        "Now we have to load the data. There's nothing to do here on your part, but this is an explanation of what is going on. First imagine splitting the dataset into N chunks where N is the batch_size and the chunks are contiguous parts of the data. For each batch, we should return one sequence from each of the chunks. The batches should also be sequential an example is described below.\n",
        "\n",
        "The data is 20 characters long `[1, 2, 3, ...20]`. The batch size is 2 and the sequence length is 4\n",
        "- The 1st batch should consist of  `(data =  [[1, 2, 3, 4]; [11, 12, 13, 14]], labels = [[2, 3, 4, 5]; [12, 13, 14, 15]])`\n",
        "- The 2nd batch should consist of `(data =  [[5, 6, 7, 8]; [15, 16, 17, 18]], labels = [[6, 7, 8, 9]; [16, 17, 18, 19]])`\n",
        "- The 3rd batch should consist of `(data =  [[9]; [19]], labels = [[10]; [20]])`\n",
        "- There is no 4th batch.\n",
        "\n",
        "Note:\n",
        "- It is OK to have one batch be shorter than the others as long as all entries in that batch are the same length.\n",
        "- The last label in one batch is the first data in the next batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v313kc-6gCnK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HarryPotterDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_file, sequence_length, batch_size):\n",
        "        super(HarryPotterDataset, self).__init__()\n",
        "\n",
        "        self.sequence_length = sequence_length\n",
        "        self.batch_size = batch_size\n",
        "        self.vocab = Vocabulary(data_file)\n",
        "\n",
        "        with open(data_file, 'rb') as data_pkl:\n",
        "            dataset = pickle.load(data_pkl)\n",
        "\n",
        "        # make dataset length a multiple of batch size\n",
        "        # removing the last bit to make the data the proper shape mostly gives better results than padding with 0s.\n",
        "        new_dataset_length = (len(dataset['tokens'])//self.batch_size) * self.batch_size        \n",
        "        self.tokens = dataset['tokens'][:new_dataset_length]\n",
        "\n",
        "        self.chunk_size = int(new_dataset_length / self.batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        # return the number of unique sequences you have, not the number of characters.\n",
        "        return self.batch_size * int(np.ceil((self.chunk_size-1)/self.sequence_length))\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        # Return the data and label for a character sequence\n",
        "        # Return a single entry for the batch using the idx to decide which chunk you are in and how far down in the chunk you are.\n",
        "        chunk_idx = idx % self.batch_size\n",
        "        pos_idx = idx // self.batch_size\n",
        "\n",
        "        start_idx = self.chunk_size * chunk_idx + self.sequence_length * pos_idx\n",
        "        end_idx = self.chunk_size * chunk_idx + min(self.sequence_length * pos_idx + self.sequence_length + 1, self.chunk_size)\n",
        "\n",
        "        data = self.tokens[start_idx : end_idx]\n",
        "        # The data and labels should be torch long tensors.\n",
        "        return torch.LongTensor(data[:-1]), torch.LongTensor(data[1:])\n",
        "\n",
        "    def vocab_size(self):\n",
        "        return len(self.vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGnjPdo-X8gu",
        "colab_type": "text"
      },
      "source": [
        "# 2.2 Defining the network\n",
        "\n",
        "The network, a generic RNN is defined below for you. Just replace the GRU layer with an LSTM layer. You can look up LSTM definition in Pytorch online. Then, implement the forward pass as instructed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8n79fvLX8g3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEMPERATURE = 0.5\n",
        "\n",
        "class HarryPotterNet(nn.Module):\n",
        "    def __init__(self, vocab_size, feature_size):\n",
        "        super(HarryPotterNet, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.feature_size = feature_size\n",
        "        self.encoder = nn.Embedding(self.vocab_size, self.feature_size)\n",
        "        # TO DO (replace the following line with LSTM layer)\n",
        "        self.gru = nn.GRU(self.feature_size, self.feature_size, batch_first=True)\n",
        "        self.decoder = nn.Linear(self.feature_size, self.vocab_size)\n",
        "        \n",
        "        # This shares the encoder and decoder weights as described in lecture.\n",
        "        self.decoder.weight = self.encoder.weight\n",
        "        self.decoder.bias.data.zero_()\n",
        "        \n",
        "        self.best_accuracy = -1\n",
        "    \n",
        "    def forward(self, x, hidden_state=None):\n",
        "        batch_size = x.shape[0]\n",
        "        sequence_length = x.shape[1]\n",
        "        \n",
        "        # TO DO \n",
        "        # pass x though the encoder first, then through LSTM, then through the decoder.\n",
        "        # return items from LSTM layer should be both the output and the hidden state.\n",
        "        raise NotImplementedError \n",
        "\n",
        "        return x, hidden_state\n",
        "\n",
        "    # This defines the function that gives a probability distribution and implements the temperature computation.\n",
        "    def inference(self, x, hidden_state=None, temperature=1):\n",
        "        x = x.view(-1, 1)\n",
        "        x, hidden_state = self.forward(x, hidden_state)\n",
        "        x = x.view(1, -1)\n",
        "        x = x / max(temperature, 1e-20)\n",
        "        x = F.softmax(x, dim=1)\n",
        "        return x, hidden_state\n",
        "\n",
        "    # Predefined loss function\n",
        "    def loss(self, prediction, label, reduction='mean'):\n",
        "        loss_val = F.cross_entropy(prediction.view(-1, self.vocab_size), label.view(-1), reduction=reduction)\n",
        "        return loss_val\n",
        "\n",
        "    # Saves the current model\n",
        "    def save_model(self, file_path, num_to_keep=1):\n",
        "        pt_util.save(self, file_path, num_to_keep)\n",
        "\n",
        "    # Saves the best model so far\n",
        "    def save_best_model(self, accuracy, file_path, num_to_keep=1):\n",
        "        if accuracy > self.best_accuracy:\n",
        "            self.save_model(file_path, num_to_keep)\n",
        "            self.best_accuracy = accuracy\n",
        "\n",
        "    def load_model(self, file_path):\n",
        "        pt_util.restore(self, file_path)\n",
        "\n",
        "    def load_last_model(self, dir_path):\n",
        "        return pt_util.restore_latest(self, dir_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl7424E4X8g7",
        "colab_type": "text"
      },
      "source": [
        "# 2.3 Character generation and training\n",
        "\n",
        "You don't have to write any code in this part. Just understand how the steps work. The training and testing functions and the main function are quite similar to the ones in Part 1. Here is an interesting article about sampling strategies (we have used max sampling strategy here for convenience):\n",
        "https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277\n",
        "\n",
        "Feel free to modify various portions of the code to understand better, and refer to online resources. Natural Language Processing details are beyond the scope of this course.\n",
        "\n",
        "At the end, save the 3 plots (train loss, test loss and test accuracy) as before and put them in your report on a single page (titled **NLP plots**). In addition, write down (in your report) the generated sentence that will be displayed at the end. You will also need to submit 'language_020.pt' file from 'checkpoints' folder in your 'language' folder.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTya80NxX8g8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEMPERATURE = 0.5\n",
        "BEAM_WIDTH = 10\n",
        "import tqdm\n",
        "\n",
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "    \n",
        "\n",
        "def max_sampling_strategy(sequence_length, model, output, hidden, vocab):\n",
        "    outputs = []\n",
        "    ind = torch.argmax(output)\n",
        "    for ii in range(sequence_length):\n",
        "        output, hidden = model.inference(ind, hidden, TEMPERATURE)\n",
        "        ind = torch.argmax(output)\n",
        "        outputs.append(ind)\n",
        "    return outputs\n",
        "\n",
        "def generate_language(model, device, seed_words, sequence_length, vocab, sampling_strategy='max', beam_width=BEAM_WIDTH):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        seed_words_arr = vocab.words_to_array(seed_words)\n",
        "\n",
        "        # Computes the initial hidden state from the prompt (seed words).\n",
        "        hidden = None\n",
        "        for ind in seed_words_arr:\n",
        "            data = ind.to(device)\n",
        "            output, hidden = model.inference(data, hidden)\n",
        "        \n",
        "        outputs = max_sampling_strategy(sequence_length, model, output, hidden, vocab)\n",
        "        return vocab.array_to_words(seed_words_arr.tolist() + outputs)\n",
        "    \n",
        "\n",
        "def train(model, device, optimizer, train_loader, lr, epoch, log_interval):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    hidden = None\n",
        "    for batch_idx, (data, label) in enumerate(tqdm.tqdm(train_loader)):\n",
        "        # send data and label to GPU\n",
        "        data, label = data.to(device), label.to(device)\n",
        "        # Separates the hidden state across batches.\n",
        "        # Otherwise the backward would try to go all the way to the beginning every time.\n",
        "        if hidden is not None:\n",
        "            hidden = repackage_hidden(hidden)\n",
        "               \n",
        "        optimizer.zero_grad()\n",
        "        # run the model and get the prediction\n",
        "        output, hidden = model(data)\n",
        "        pred = output.max(-1)[1]\n",
        "        \n",
        "        # compute and backpropagate the loss\n",
        "        loss = model.loss(output, label)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "    return np.mean(losses)\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden = None\n",
        "        for batch_idx, (data, label) in enumerate(test_loader):\n",
        "            # send data and label to GPU\n",
        "            data, label = data.to(device), label.to(device)\n",
        "            \n",
        "            # test the model\n",
        "            output, hidden = model(data, hidden)\n",
        "            pred = output.max(-1)[1]\n",
        "            correct_mask = pred.eq(label.view_as(pred))\n",
        "            num_correct = correct_mask.sum().item()\n",
        "            correct += num_correct\n",
        "            \n",
        "            # compute test loss\n",
        "            test_loss += model.loss(output, label, reduction='mean').item()\n",
        "            \n",
        "            # Comment this out to avoid printing test results\n",
        "            if batch_idx % 10 == 0:\n",
        "                print('Input\\t%s\\nGT\\t%s\\npred\\t%s\\n\\n' % (\n",
        "                    test_loader.dataset.vocab.array_to_words(data[0]),\n",
        "                    test_loader.dataset.vocab.array_to_words(label[0]),\n",
        "                    test_loader.dataset.vocab.array_to_words(pred[0])))\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    test_accuracy = 100. * correct / (len(test_loader.dataset) * test_loader.dataset.sequence_length)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset) * test_loader.dataset.sequence_length,\n",
        "        100. * correct / (len(test_loader.dataset) * test_loader.dataset.sequence_length)))\n",
        "    return test_loss, test_accuracy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hljrTyRmX8g-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    SEQUENCE_LENGTH = 100\n",
        "    BATCH_SIZE = 256\n",
        "    FEATURE_SIZE = 512\n",
        "    TEST_BATCH_SIZE = 256\n",
        "    EPOCHS = 20\n",
        "    LEARNING_RATE = 0.002\n",
        "    WEIGHT_DECAY = 0.0005\n",
        "    USE_CUDA = True\n",
        "    PRINT_INTERVAL = 10\n",
        "    LOG_PATH = DATA_PATH + 'logs/log.pkl'\n",
        "\n",
        "    # setup GPU and multiprocessing\n",
        "    use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    print('Using device', device)\n",
        "    import multiprocessing\n",
        "    num_workers = multiprocessing.cpu_count()\n",
        "    print('num workers:', num_workers)\n",
        "    kwargs = {'num_workers': num_workers,'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "    # get train and test data\n",
        "    data_train = HarryPotterDataset(DATA_PATH + 'harry_potter_chars_train.pkl', SEQUENCE_LENGTH, BATCH_SIZE)\n",
        "    data_test = HarryPotterDataset(DATA_PATH + 'harry_potter_chars_test.pkl', SEQUENCE_LENGTH, TEST_BATCH_SIZE)\n",
        "    vocab = data_train.vocab\n",
        "    train_loader = torch.utils.data.DataLoader(data_train, batch_size=BATCH_SIZE, shuffle=False, **kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(data_test, batch_size=TEST_BATCH_SIZE, shuffle=False, **kwargs)\n",
        "\n",
        "    # initialize model\n",
        "    model = HarryPotterNet(data_train.vocab_size(), FEATURE_SIZE).to(device)\n",
        "\n",
        "    # Adam is an optimizer like SGD but a bit fancier. It tends to work faster and better than SGD.\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    \n",
        "    # This will train from scratch\n",
        "    start_epoch = 0 \n",
        "    train_losses, test_losses, test_accuracies = [], [], []\n",
        "\n",
        "    # To resume training from last saved model, uncomment the following 2 lines\n",
        "    # start_epoch = model.load_last_model(DATA_PATH + 'checkpoints')\n",
        "    # train_losses, test_losses, test_accuracies = pt_util.read_log(LOG_PATH, ([], [], []))\n",
        "\n",
        "    # Get initial test loss and accuracy\n",
        "    test_loss, test_accuracy = test(model, device, test_loader)\n",
        "    test_losses.append((start_epoch, test_loss))\n",
        "    test_accuracies.append((start_epoch, test_accuracy))\n",
        "\n",
        "    try:\n",
        "        for epoch in range(start_epoch, EPOCHS + 1):\n",
        "            lr = LEARNING_RATE * np.power(0.25, (int(epoch / 6)))\n",
        "            \n",
        "            # train for 1 epoch\n",
        "            train_loss = train(model, device, optimizer, train_loader, lr, epoch, PRINT_INTERVAL)\n",
        "            train_losses.append((epoch, train_loss))\n",
        "            # test after 1 epoch\n",
        "            test_loss, test_accuracy = test(model, device, test_loader)\n",
        "            test_losses.append((epoch, test_loss))\n",
        "            test_accuracies.append((epoch, test_accuracy))\n",
        "            # Log the losses and accuracy\n",
        "            pt_util.write_log(LOG_PATH, (train_losses, test_losses, test_accuracies))\n",
        "            # save best model\n",
        "            model.save_best_model(test_accuracy, DATA_PATH + 'checkpoints/language_%03d.pt' % epoch)\n",
        "            \n",
        "            # Test with a sentence starting with the given seed words. See what is generated next.\n",
        "            seed_words = 'Harry Potter, Voldemort, and Dumbledore walk into a bar. '\n",
        "            generated_sentence = generate_language(model, device, seed_words, 200, vocab, 'max')\n",
        "            print('generated sentence\\t\\t', generated_sentence)\n",
        "            print('')\n",
        "\n",
        "    except KeyboardInterrupt as ke:\n",
        "        print('Interrupted')\n",
        "    except:\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "    finally:\n",
        "        print('Saving final model')\n",
        "        model.save_model(DATA_PATH + 'checkpoints/language_%03d.pt' % epoch, 0)\n",
        "        # plot the loss and accuracy values\n",
        "        ep, val = zip(*train_losses)\n",
        "        pt_util.plot(ep, val, 'Train loss', 'Epoch', 'Error', name)\n",
        "        ep, val = zip(*test_losses)\n",
        "        pt_util.plot(ep, val, 'Test loss', 'Epoch', 'Error', name)\n",
        "        ep, val = zip(*test_accuracies)\n",
        "        pt_util.plot(ep, val, 'Test accuracy', 'Epoch', 'Error', name)\n",
        "        return model, vocab, device\n",
        "\n",
        "final_model, vocab, device = main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kwAXWXg8jzS4"
      },
      "source": [
        "# Submission.\n",
        "Download a copy of this python notebook using File -> download .ipynb. Upload the following files on Canvas INDIVIDUALLY (**Do Not ZIP**):\n",
        "\n",
        "- cse415A6.ipynb\n",
        "- report.pdf\n",
        "- cifar_050.pt\n",
        "- language_020.pt\n",
        "\n",
        "P.S. This codebase has been written following the codebase for the homework assignments of CSE499G1 / CSE599G1 (Introduction to Deep Learning) course during Fall 2019."
      ]
    }
  ]
}